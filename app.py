# app.py  â€” PDF ê¸°ë°˜ Q&A (ê°•í™” RAG, ë¹„ìš© ìµœì í™” + ê²¬ê³  ì¶œë ¥)
import os, re, json, tempfile, hashlib, pathlib
from typing import List, Tuple

from dotenv import load_dotenv
load_dotenv(override=True)

import streamlit as st
import pandas as pd

# LangChain / OpenAI
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
try:
    from langchain_community.document_loaders import PyPDFLoader
except ModuleNotFoundError:
    from langchain_community.document_loaders.pdf import PyPDFLoader  # ì•„ì£¼ êµ¬ë²„ì „ ëŒ€ë¹„

from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# (ì„ íƒ) ì»¨í…ìŠ¤íŠ¸ ì••ì¶•
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor

# =========================
# ê¸°ë³¸ UI
# =========================
st.set_page_config(page_title="PDF QA (ê°•í™” RAG)", page_icon="ğŸ“„", layout="centered")
st.title("ğŸ“„ PDF ê¸°ë°˜ Q&A (ìš”ì•½í‘œ â†’ ì§ˆë¬¸/ë‹µë³€, ê°•í™” RAG)")

# =========================
# API í‚¤
# =========================
def get_openai_key():
    key = os.getenv("OPENAI_API_KEY")
    if key:
        return key
    try:
        return st.secrets["OPENAI_API_KEY"]
    except Exception:
        return None

OPENAI_API_KEY = get_openai_key()
if not OPENAI_API_KEY:
    st.error("OPENAI_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤. (.env ë˜ëŠ” .streamlit/secrets.toml / í´ë¼ìš°ë“œ Secrets ì„¤ì •)")
    st.stop()

# ===== ëª¨ë¸ ì„¤ì • (í™˜ê²½ë³€ìˆ˜ë¡œ ì˜¤ë²„ë¼ì´ë“œ ê°€ëŠ¥) =====
PRIMARY_MODEL = os.getenv("OPENAI_CHAT_MODEL_PRIMARY", "gpt-5-mini")   # ë©”ì¸ ìƒì„±/ì¶”ë¡ 
LIGHT_MODEL   = os.getenv("OPENAI_CHAT_MODEL_LIGHT",   "gpt-5-nano")   # ì••ì¶•/ì •ì œ/ê°„ë‹¨ íƒœìŠ¤í¬
EMBED_MODEL   = os.getenv("OPENAI_EMBED_MODEL",         "text-embedding-3-small")  # ìµœì €ê°€ ì„ë² ë”©

# =========================
# ë¹„ìš© ìµœì í™” ê¸°ë³¸ê°’ (ìŠ¬ë¼ì´ë”)
# =========================
with st.sidebar:
    st.subheader("ğŸ”§ ì¸ë±ì‹±")
    chunk_size = st.slider("Chunk size", 300, 2000, 900, 50)
    chunk_overlap = st.slider("Chunk overlap", 0, 400, 150, 10)
    st.caption("ë„ˆë¬´ ì‘ìœ¼ë©´ ì²­í¬ ìˆ˜â†‘(ì„ë² ë”© ë¹„ìš©â†‘), ë„ˆë¬´ í¬ë©´ ê²€ìƒ‰ ì •ë°€ë„â†“")

    st.subheader("ğŸ” ê²€ìƒ‰/ìƒì„±")
    top_k = st.slider("ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜ k", 1, 10, 5, 1)
    fetch_k = st.slider("í›„ë³´ fetch_k", 10, 60, 20, 2)
    use_compression = st.checkbox("ì»¨í…ìŠ¤íŠ¸ ì••ì¶• ì‚¬ìš©(ê¶Œì¥, ë¹„ìš©â†“Â·ì •í™•ë„â†‘)", True)
    temperature = st.slider("temperature", 0.0, 1.0, 0.2, 0.1)
    max_tokens = st.slider("ì‘ë‹µ í† í° ìƒí•œ", 200, 1200, 450, 50)

    st.subheader("ğŸ§  ë¹„íŒ/ë¦¬ìŠ¤í¬ ì§ˆë¬¸ ì²˜ë¦¬(ì „ì—­ ìŠ¤ìº”)")
    enable_global_critique = st.checkbox("ë¹„íŒí˜• ì§ˆë¬¸ì— ì „ì—­ ìŠ¤ìº” ì‚¬ìš©", True)
    global_critique_pages = st.slider("ì „ì—­ ìŠ¤ìº”: í˜ì´ì§€ë‹¹ ë°œì·Œ ê¸¸ì´", 200, 1200, 700, 50)
    global_critique_total = st.slider("ì „ì—­ ìŠ¤ìº”: ì „ì²´ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´", 4000, 20000, 12000, 500)

    st.subheader("ğŸ§­ ì»¨ì„¤íŒ… ëª¨ë“œ (íŒ”ë€í‹°ì–´ ìŠ¤íƒ€ì¼)")
    strategy_mode = st.checkbox("ì»¨ì„¤íŒ… ëª¨ë“œ í™œì„±í™”", False)
    strat_goal = st.text_input("í•µì‹¬ ëª©í‘œ(KPI/ë§¤ì¶œì§€í‘œ)", "ì›” êµ¬ë… ë§¤ì¶œ MRR ì¦ëŒ€")
    strat_horizon = st.selectbox("ì‹œê°„ì¶•", ["90ì¼", "180ì¼", "12ê°œì›”"], index=0)
    strat_budget = st.text_input("ì˜ˆì‚°/ì œì•½(ì„ íƒ)", "ì œí•œì  ì¸ë ¥ 2ëª…, ë§ˆì¼€íŒ… ì›” 1ì²œë§Œ ì›")
    strat_segment = st.text_input("í•µì‹¬ ì„¸ê·¸ë¨¼íŠ¸(ì„ íƒ)", "ì´ˆë³´ ì‹ì§‘ì‚¬, í”„ë¦¬ë¯¸ì—„ êµ¬ë…ì")
    strat_competitors = st.text_input("ê²½ìŸì‚¬/ëŒ€ì²´ì¬(ì„ íƒ)", "í”ŒëœíŠ¸ID, ë„¤ì´ë²„ ì‹ë¬¼ ì»¤ë®¤ë‹ˆí‹°")

uploaded = st.file_uploader("PDF ì—…ë¡œë“œ", type=["pdf"])
question = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: 'ì˜ëœ ì  3ê°€ì§€ì™€ ë¶€ì¡±í•œ ì  3ê°€ì§€ë¥¼ ì•Œë ¤ì¤˜')")

# =========================
# ìœ í‹¸ (ì˜ë„Â·ê°œìˆ˜ íŒŒì‹± / ë¹„ìš©Â·ìºì‹œ)
# =========================
PROS_KEYS = ["ì˜ëœ", "ì˜ ëœ", "ì¥ì ", "ê°•ì ", "ì¢‹ì€ ì "]
CONS_KEYS = ["ë¶€ì¡±", "ë‹¨ì ", "í•œê³„", "ë¦¬ìŠ¤í¬", "ë¬¸ì œì ", "ì·¨ì•½", "ë¶ˆí¸", "ì œì•½", "ìœ„í—˜", "ë³´ì™„"]

def extract_count(q: str, default=3) -> int:
    m = re.search(r"(\d+)\s*ê°€ì§€", q or "")
    return int(m.group(1)) if m else default

def detect_intent(q: str):
    ql = (q or "").lower()
    has_pros = any(k in ql for k in [*PROS_KEYS, "pros", "advantages"])
    has_cons = any(k in ql for k in [*CONS_KEYS, "cons", "risks"])
    if has_pros and has_cons: return "pros_cons"  # ì¥/ë‹¨ì  í˜¼í•©
    if has_cons: return "critique"               # ë‹¨ì /ë¦¬ìŠ¤í¬ ì¤‘ì‹¬
    return "general"

def looks_like_critique(q: str) -> bool:
    return detect_intent(q) == "critique"

def sha256_bytes(b: bytes) -> str:
    return hashlib.sha256(b).hexdigest()[:16]

def save_dir_for(hash_key: str) -> str:
    base = pathlib.Path(".faiss_cache")
    base.mkdir(parents=True, exist_ok=True)
    return str(base / f"vs_{hash_key}")

def extract_page_citations(text: str) -> List[int]:
    pages = set(int(p) for p in re.findall(r"\[p\.(\d+)\]", text or ""))
    return sorted(list(pages))

# ---- ê²¬ê³ í•œ JSON íŒŒì„œ ----
def safe_json_loads(s: str, allow_empty: bool = False):
    if not s or not s.strip():
        if allow_empty:
            return {}
        raise ValueError("empty JSON response")
    s = s.strip()
    if s.startswith("```"):
        s = re.sub(r"^```(?:json)?\s*|\s*```$", "", s, flags=re.DOTALL)
    m = re.search(r"\{[\s\S]*\}", s)
    if m: s = m.group(0)
    if "'" in s and '"' not in s:
        s = s.replace("'", '"')
    s = re.sub(r"(?m)^\s*//.*$", "", s)
    try:
        return json.loads(s)
    except Exception:
        if allow_empty:
            return {}
        raise

# ---- LLM ì‘ë‹µì—ì„œ í…ìŠ¤íŠ¸ ê°•ì œ ì¶”ì¶œ (tool_calls ëŒ€ë¹„) ----
def to_text(resp) -> str:
    """
    BaseMessageì—ì„œ contentê°€ ë¹„ì—ˆì„ ë•Œ tool_calls/function_call.argumentsì—ì„œ ë³¸ë¬¸ íšŒìˆ˜.
    í•­ìƒ ë¬¸ìì—´ ë°˜í™˜.
    """
    if resp is None:
        return ""
    try:
        text = (getattr(resp, "content", None) or "").strip()
        if text:
            return text
        ak = getattr(resp, "additional_kwargs", {}) or {}
        # OpenAI tool_calls
        tcs = ak.get("tool_calls") or []
        if tcs:
            fn = (tcs[0] or {}).get("function", {})
            args = fn.get("arguments", "") or ""
            return str(args).strip()
        # legacy function_call
        fc = ak.get("function_call") or {}
        if fc:
            return str(fc.get("arguments", "") or "").strip()
        return ""
    except Exception:
        return ""

# =========================
# ì¸ë±ìŠ¤ ìƒì„±/ë¡œë”© (ìºì‹œ + ë””ìŠ¤í¬ ì €ì¥)
# =========================
@st.cache_resource(show_spinner=False)
def load_or_build_index(pdf_bytes: bytes, chunk_size:int, chunk_overlap:int, api_key:str):
    pdf_hash = sha256_bytes(pdf_bytes)
    folder = save_dir_for(pdf_hash)

    if os.path.isdir(folder) and os.path.exists(os.path.join(folder, "index.faiss")):
        embeddings = OpenAIEmbeddings(api_key=api_key, model=EMBED_MODEL)
        vs = FAISS.load_local(folder, embeddings, allow_dangerous_deserialization=True)
    else:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
            tmp.write(pdf_bytes)
            path = tmp.name
        docs = PyPDFLoader(path).load()

        splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size, chunk_overlap=chunk_overlap, separators=["\n\n", "\n", " ", ""]
        )
        splits = splitter.split_documents(docs)

        embeddings = OpenAIEmbeddings(api_key=api_key, model=EMBED_MODEL)
        vs = FAISS.from_documents(splits, embeddings)
        vs.save_local(folder)

    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
        tmp.write(pdf_bytes)
        path2 = tmp.name
    page_docs = PyPDFLoader(path2).load()
    pages = [(d.metadata.get("page", 0)+1, d.page_content) for d in page_docs]
    return vs, pages

def summarize_corpus(retriever, pages, max_chars=10000):
    """PDF ê¸°ë°˜ í•µì‹¬ ì‚¬ì‹¤ ìš”ì•½(ì‹œì¥/ì œí’ˆ/ìˆ˜ìµ/ê°•ì•½ì /ë¦¬ìŠ¤í¬) + í˜ì´ì§€ ê·¼ê±° í¬í•¨"""
    # ì „ì—­ + êµ­ì†Œ ì„ê¸° (ë¹„ìš© ì ˆì•½ì„ ìœ„í•´ ìƒí•œ ì ìš©)
    context = []
    if pages:
        for p, txt in pages[:10]:  # ì•ìª½ 10í˜ì´ì§€ë§Œ ìŠ¬ë¦¼í•˜ê²Œ
            context.append(f"[p.{p}] {txt[:600]}")
    docs = retriever.get_relevant_documents("ì„œë¹„ìŠ¤ ê°œìš”, ì‹œì¥, ìˆ˜ìµëª¨ë¸, ê°•ì /ì•½ì , ë¦¬ìŠ¤í¬, ì‹¤í–‰ ê³„íš")
    context.extend([f"[p.{d.metadata.get('page',0)+1}] {d.page_content}" for d in docs])
    context = "\n\n".join(context)[:max_chars]

    sys = (
        "ë„ˆëŠ” ìë£Œì¡°ì‚¬ ë¶„ì„ê°€ë‹¤. CONTEXTì—ì„œ ë‹¤ìŒì„ í•­ëª©ë³„ë¡œ ìš”ì•½í•˜ë¼."
        "\n1) ì‹œì¥/ê³ ê°\n2) ì œí’ˆ/í•µì‹¬ê°€ì¹˜\n3) ìˆ˜ìµëª¨ë¸\n4) ê°•ì \n5) ì•½ì /ë¦¬ìŠ¤í¬\n6) í˜„ì¬ ê°€ì •Â·ì œì•½"
        "\nê° bullet ëì— [p.x] ê·¼ê±°ë¥¼ ë¶™ì—¬ë¼. í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê²Œ."
    )
    user = "CONTEXT:\n" + context
    return to_text(llm_light(max_tokens=650, temperature=0).invoke(
        [{"role":"system","content":sys},{"role":"user","content":user}]
    ))

def analyze_strategy(brief:str, goal:str, horizon:str, budget:str, segment:str, competitors:str):
    """ì»¨ì„¤íŒ… ëª¨ë“œ: ìˆ˜ì¹˜/ë¡œë“œë§µ/KPI/ë¦¬ìŠ¤í¬ê¹Œì§€ êµ¬ì¡°í™”ëœ ì „ëµ ë¦¬í¬íŠ¸(JSON) ìƒì„±"""
    sys = (
        "ë„ˆëŠ” ë°ì´í„° ì¤‘ì‹¬ ì „ëµ ì»¨ì„¤í„´íŠ¸ë‹¤. ì•„ë˜ BRIEF(ìë£Œìš”ì•½)ë¥¼ ë°”íƒ•ìœ¼ë¡œ "
        "ë§¤ì¶œ ê·¹ëŒ€í™” ì „ëµì„ **ìˆ˜ì¹˜ ê°€ì„¤ê³¼ ì‹¤í–‰ ë¡œë“œë§µ**ìœ¼ë¡œ ì œì‹œí•˜ë¼. "
        "ê° ì œì•ˆì€ ëª…í™•í•œ ê°€ì •/ê·¼ê±°ì™€ ë¦¬ìŠ¤í¬/ëŒ€ì‘ì„ í¬í•¨í•´ì•¼ í•œë‹¤. "
        "ì¶œë ¥ì€ JSON í•œ ê°œë§Œ ë°˜í™˜í•˜ë¼."
    )
    schema = (
        '{'
        '"summary":"ê³ ìˆ˜ì¤€ ìš”ì•½",'
        '"north_star":"í•µì‹¬ ëª©í‘œ/KPI",'
        '"assumptions":["ê°€ì • ..."],'
        '"strategies":[{'
            '"name":"ì „ëµëª…",'
            '"rationale":"ì™œ ì´ ì „ëµì¸ì§€",'
            '"expected_impact":{"metric":"ì§€í‘œëª…","baseline":"í˜„ì¬ê°’","target":"ëª©í‘œê°’","lift_pct":10},'
            '"actions":{"90d":["ì•¡ì…˜1","ì•¡ì…˜2"],"180d":["..."],"12m":["..."]},'
            '"kpis":["KPI1","KPI2"],'
            '"risks":[{"risk":"ìœ„í—˜","mitigation":"ëŒ€ì‘"}],'
            '"evidence":["[p.x]","[p.y]"]'
        '}],'
        '"data_needs":["ì¶”ê°€ë¡œ í•„ìš”í•œ ë°ì´í„°/ì‹¤í—˜"]'
        '}'
    )
    user = (
        f"BRIEF:\n{brief}\n\n"
        f"ëª©í‘œ(North Star): {goal}\n"
        f"ì‹œê°„ì¶•: {horizon}\n"
        f"ì˜ˆì‚°/ì œì•½: {budget}\n"
        f"í•µì‹¬ ì„¸ê·¸ë¨¼íŠ¸: {segment}\n"
        f"ê²½ìŸì‚¬: {competitors}\n\n"
        f"ìœ„ ì¡°ê±´ì„ ë°˜ì˜í•´ JSON ìŠ¤í‚¤ë§ˆ {schema} ë¡œ ë°˜í™˜í•˜ë¼. "
        "ê° ìˆ˜ì¹˜ ëª©í‘œëŠ” ë³´ìˆ˜ì /í˜„ì‹¤ì  ê°€ì •ìœ¼ë¡œ ì¶”ì •ì¹˜ë¥¼ ì œì‹œí•˜ë¼(ì˜ˆ: ì „í™˜ìœ¨ +x%p, ARPU +y%). "
        "ë¬¸ì„œ ê·¼ê±°ëŠ” evidenceì— [p.x] í˜•íƒœë¡œ í¬í•¨."
    )
    # JSON ëª¨ë“œ ê°•ì œ
    raw = to_text(llm_chat_json(max_tokens=900, temperature=0).invoke(
        [{"role":"system","content":sys},{"role":"user","content":user}]
    ))
    data = safe_json_loads(raw, allow_empty=True)
    if not isinstance(data, dict) or not data:
        # ë§ˆì§€ë§‰ í´ë°±: ë§ˆí¬ë‹¤ìš´
        return {
            "summary": "ë¶„ì„ ê²°ê³¼(í…ìŠ¤íŠ¸ í´ë°±)",
            "strategies": [{"name":"ì „ëµ ì œì•ˆ ìƒì„± ì‹¤íŒ¨", "rationale": raw[:400] or "ë¹ˆ ì‘ë‹µ"}],
        }
    return data


def _format_docs(docs, max_chars=3000):
    text = "\n\n".join(f"[p.{(d.metadata.get('page',0)+1)}] {d.page_content}" for d in docs)
    return text[:max_chars]

# =========================
# LLM í˜¸ì¶œ í—¬í¼ (5 mini / 5 nano / JSON ëª¨ë“œ)
# =========================
def llm_chat(max_tokens:int=600, temperature:float=0.2):
    return ChatOpenAI(api_key=OPENAI_API_KEY, model=PRIMARY_MODEL,
                      temperature=temperature, max_tokens=max_tokens)

def llm_light(max_tokens:int=300, temperature:float=0):
    return ChatOpenAI(api_key=OPENAI_API_KEY, model=LIGHT_MODEL,
                      temperature=temperature, max_tokens=max_tokens)

def llm_chat_json(max_tokens:int=700, temperature:float=0):
    # json_object ê°•ì œ + ì‹¤íŒ¨ ì‹œ ì¼ë°˜ëª¨ë“œë¡œ JSONë§Œ ì¶œë ¥í•˜ë„ë¡ í´ë°±
    def _invoke(msgs):
        try:
            return ChatOpenAI(
                api_key=OPENAI_API_KEY,
                model=PRIMARY_MODEL,
                temperature=temperature,
                max_tokens=max_tokens,
                extra_body={"response_format": {"type": "json_object"}},
            ).invoke(msgs).content
        except Exception:
            hard_sys = (
                'ë‹¤ìŒ ìš”ì²­ì— ëŒ€í•´ {"pros":["..."],"cons":["..."]} í˜•ì‹ì˜ **ìœ íš¨í•œ JSON í•œ ê°œë§Œ** ì¶œë ¥í•˜ë¼. '
                "ê·¸ ì™¸ í…ìŠ¤íŠ¸/ì½”ë“œë¸”ë¡/ì„¤ëª… ê¸ˆì§€."
            )
            return ChatOpenAI(
                api_key=OPENAI_API_KEY,
                model=PRIMARY_MODEL,
                temperature=0,
                max_tokens=max_tokens
            ).invoke(
                [{"role":"system","content":hard_sys}] + msgs
            ).content
    class _Runner:
        def invoke(self, msgs):
            return type("resp", (), {"content": _invoke(msgs)})()
    return _Runner()

# =========================
# pros/cons ì „ìš© ë£¨íŠ¸: JSON â†’ ë§ˆí¬ë‹¤ìš´ (3ì¤‘ ì•ˆì „ë§)
# =========================
def answer_pros_cons(question: str, retriever, pages, n: int, use_global: bool,
                     per_page_chars:int, total_chars:int, api_key:str, max_tokens:int=750) -> str:
    # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    if use_global and pages:
        buf = [f"[p.{p}] {txt[:per_page_chars]}" for p, txt in pages]
        context = "\n\n".join(buf)[:total_chars]
    else:
        docs = retriever.get_relevant_documents(question)
        context = "\n\n".join(
            f"[p.{d.metadata.get('page',0)+1}] {d.page_content}" for d in docs
        )[: min(12000, total_chars)]

    sys = (
        "ë‹¤ìŒ CONTEXTì— ê·¼ê±°í•´ ì‚¬ìš©ìì˜ ì§ˆë¬¸(ì¥ì ê³¼ ë‹¨ì ì„ ê°ê° Nê°œ)ì„ ì •í™•íˆ ë”°ë¥¸ë‹¤. "
        "ê° í•­ëª©ì˜ ë¬¸ì¥ ëì— ë°˜ë“œì‹œ [p.x] ê·¼ê±°ë¥¼ í¬í•¨í•œë‹¤. "
        'ì¶œë ¥ì€ {"pros":["... [p.x]"],"cons":["... [p.y]"]} í˜•ì‹ì˜ JSON **í•œ ê°œ**ë§Œ ë°˜í™˜í•˜ë¼.'
    )
    user = f"N={n}\nì§ˆë¬¸: {question}\n\nCONTEXT:\n{context}"

    # 1) JSON ëª¨ë“œ 1ì°¨ + ë¯¸ì„¸ ì¬ì‹œë„
    data = {}
    out = ""
    for _ in range(2):
        out = to_text(
            llm_chat_json(max_tokens=max_tokens, temperature=0).invoke(
                [{"role":"system","content":sys},{"role":"user","content":user}]
            )
        )
        data = safe_json_loads(out, allow_empty=True)
        if data:
            break

    # 2) ê²½ëŸ‰(nano)ë¡œ JSON ì¶”ì¶œ/ìˆ˜ì •
    if not data:
        out2 = to_text(
            llm_light(max_tokens=300, temperature=0).invoke(
                [
                    {"role":"system","content":"ì•„ë˜ í…ìŠ¤íŠ¸ì—ì„œ í•„ìš”í•œ JSONë§Œ ì¶”ì¶œ/ìˆ˜ì •í•˜ì—¬ ìœ íš¨í•œ JSON í•œ ê°œë¡œ ë°˜í™˜í•˜ë¼."},
                    {"role":"user","content":out or ""},
                ]
            )
        )
        data = safe_json_loads(out2, allow_empty=True)

    # 3) ê·¸ë˜ë„ ì‹¤íŒ¨ â†’ ë§ˆí¬ë‹¤ìš´ ì§ì ‘ ìƒì„±
    if not isinstance(data, dict) or ("pros" not in data and "cons" not in data):
        md = to_text(
            llm_chat(max_tokens=max_tokens).invoke(
                [
                    {"role":"system","content":f"ì¥ì  {n}ê°œì™€ ë‹¨ì  {n}ê°œë¥¼ ê°ê° bulletë¡œ ìƒì„±í•˜ë¼. ê° bullet ëì— [p.x]ë¥¼ ë¶™ì—¬ë¼. ë¬¸ì„œ ì™¸ ì¶”ì¸¡ ê¸ˆì§€."},
                    {"role":"user","content":f"ì§ˆë¬¸: {question}\n\nCONTEXT:\n{context}"},
                ]
            )
        )
        return md

    pros = list(data.get("pros", []))[:n]
    cons = list(data.get("cons", []))[:n]
    if not pros: pros = ["ë¬¸ì„œ ê¸°ë°˜ ì¥ì  ìš”ì•½ [p.?]"]
    if not cons: cons = ["ë¬¸ì„œ ê¸°ë°˜ ë¶€ì¡±/ë¦¬ìŠ¤í¬ ìš”ì•½ [p.?]"]

    return "\n".join([
        "#### âœ… ì˜ëœ ì ", *[f"- {p}" for p in pros], "",
        "#### âš ï¸ ë¶€ì¡±í•œ ì ", *[f"- {c}" for c in cons]
    ])

# =========================
# ì „ì—­ ìŠ¤ìº”(ë¹„íŒí˜•): ì§ˆë¬¸ì„ ê·¸ëŒ€ë¡œ ë”°ë¦„
# =========================
def critique_answer_global(pages: List[Tuple[int,str]], per_page_chars:int, total_chars:int,
                           api_key:str, question:str, max_tokens:int=700) -> str:
    buf = [f"[p.{p}] {txt[:per_page_chars]}" for p, txt in pages]
    context = "\n\n".join(buf)[:total_chars]
    sys = (
        "ë‹¤ìŒ CONTEXTì— ê·¼ê±°í•´ **ì‚¬ìš©ì ì§ˆë¬¸ì„ ì •í™•íˆ ë”°ë¥¸ë‹¤**. "
        "ê°€ëŠ¥í•˜ë©´ bulletë¡œ ê°„ê²°íˆ ì„œìˆ í•˜ê³ , ê° í•­ëª© ëì— [p.x]ë¥¼ ë¶™ì¸ë‹¤. "
        "ë¬¸ì„œ ì™¸ ì¶”ì¸¡ ê¸ˆì§€."
    )
    user = f"ì§ˆë¬¸: {question}\n\nCONTEXT:\n{context}"
    return to_text(
        llm_chat(max_tokens=max_tokens).invoke(
            [{"role":"system","content":sys},{"role":"user","content":user}]
        )
    )

# =========================
# ìƒíƒœ
# =========================
if "vs" not in st.session_state: st.session_state.vs = None
if "pages" not in st.session_state: st.session_state.pages = None
if "digest_df" not in st.session_state: st.session_state.digest_df = None

# =========================
# ì—…ë¡œë“œ ì²˜ë¦¬
# =========================
if uploaded is not None and (st.session_state.get("uploaded_name") != uploaded.name):
    with st.spinner("PDF ì¸ë±ì‹±/ìºì‹œ ì¤€ë¹„ ì¤‘..."):
        vs, pages = load_or_build_index(uploaded.read(), chunk_size, chunk_overlap, OPENAI_API_KEY)
        st.session_state.vs = vs
        st.session_state.pages = pages
        st.session_state.uploaded_name = uploaded.name
        st.session_state.digest_df = None
    st.success("ì¸ë±ì‹± ì™„ë£Œ!")

# =========================
# ì¼ë°˜ ì²´ì¸ ë¹Œë” (ì§ˆë¬¸ ìš°ì„ Â·ì¤‘ë¦½ í”„ë¡¬í”„íŠ¸)
# =========================
def build_chain(retriever):
    system = (
        "ë„ˆëŠ” ì—…ë¡œë“œëœ PDFì— ê·¼ê±°í•´ í•œêµ­ì–´ë¡œ ë‹µí•œë‹¤. "
        "í•­ìƒ **ì‚¬ìš©ì ì§ˆë¬¸ì˜ ì˜ë„ì™€ ê°œìˆ˜ ìš”êµ¬ë¥¼ ì •í™•íˆ ë”°ë¥¸ë‹¤**. "
        "ê°€ëŠ¥í•˜ë©´ bulletì„ ì‚¬ìš©í•˜ê³ , ê° í•µì‹¬ ì£¼ì¥ ëì— [p.í˜ì´ì§€] ê·¼ê±°ë¥¼ ë¶™ì¸ë‹¤. "
        "ë¬¸ì„œì™€ ë¬´ê´€í•œ ì¶”ì¸¡ì€ ê¸ˆì§€í•œë‹¤."
    )
    prompt = ChatPromptTemplate.from_messages(
        [("system", system + "\n\nCONTEXT:\n{context}"), ("human", "{question}")]
    )
    chain = (
        {"context": retriever | _format_docs, "question": RunnablePassthrough()}
        | prompt
        | llm_chat(max_tokens=max_tokens, temperature=temperature)
        | StrOutputParser()
    )
    return chain

def render_strategy_md(data:dict) -> str:
    if not isinstance(data, dict): return str(data)
    def _arr(a): return a if isinstance(a, list) else []
    buf = []
    if data.get("summary"): buf += ["### ê°œìš”", data["summary"], ""]
    if data.get("north_star"): buf += [f"**North Star:** {data['north_star']}", ""]
    if _arr(data.get("assumptions")):
        buf += ["### ì£¼ìš” ê°€ì •", *[f"- {x}" for x in data["assumptions"]], ""]
    if _arr(data.get("strategies")):
        buf += ["### í•µì‹¬ ì „ëµ"]
        for i, s in enumerate(data["strategies"], 1):
            buf += [f"#### {i}. {s.get('name','ì „ëµ')}",
                    f"- **ê·¼ê±°**: {s.get('rationale','')}",
                    f"- **ì„íŒ©íŠ¸ ê°€ì„¤**: {s.get('expected_impact',{})}",
                    "- **ì‹¤í–‰(90d)**: " + ", ".join(_arr(s.get('actions',{}).get('90d',[]))),
                    "- **ì‹¤í–‰(180d)**: " + ", ".join(_arr(s.get('actions',{}).get('180d',[]))),
                    "- **ì‹¤í–‰(12m)**: " + ", ".join(_arr(s.get('actions',{}).get('12m',[]))),
                    "- **KPI**: " + ", ".join(_arr(s.get('kpis'))),
                    "- **ë¦¬ìŠ¤í¬/ëŒ€ì‘**: " + "; ".join([f"{r.get('risk')}: {r.get('mitigation')}" for r in _arr(s.get('risks'))]),
                    "- **ê·¼ê±°**: " + ", ".join(_arr(s.get('evidence'))),
                    ""]
    if _arr(data.get("data_needs")):
        buf += ["### ì¶”ê°€ ë°ì´í„°/ì‹¤í—˜ í•„ìš”", *[f"- {x}" for x in data["data_needs"]]]
    return "\n".join(buf)


# =========================
# Q&A ë¼ìš°íŒ… (ì˜ë„ë³„)
# =========================
if st.session_state.vs:
    st.markdown("---")
    st.markdown("### ğŸ’¬ ì§ˆë¬¸/ë‹µë³€")

    if st.button("ì§ˆë¬¸í•˜ê¸°") and question:
        with st.spinner("ìƒê° ì¤‘..."):
            intent = detect_intent(question)
            n = extract_count(question, 3)

            # ë¦¬íŠ¸ë¦¬ë²„ (MMR + ì••ì¶•)
            base_retriever = st.session_state.vs.as_retriever(
                search_type="mmr", search_kwargs={"k": top_k, "fetch_k": fetch_k}
            )
            retriever = base_retriever
            if use_compression:
                compressor = LLMChainExtractor.from_llm(
                    llm_light(max_tokens=300, temperature=0)  # ì••ì¶•ì€ nanoë¡œ ë¹„ìš©â†“
                )
                retriever = ContextualCompressionRetriever(
                    base_retriever=base_retriever, base_compressor=compressor
                )
            
                        # === NEW: ì»¨ì„¤íŒ… ëª¨ë“œ ===
            if strategy_mode:
                # ë¦¬íŠ¸ë¦¬ë²„ ì¤€ë¹„ (MMR + ì••ì¶•)
                base_retriever = st.session_state.vs.as_retriever(
                    search_type="mmr", search_kwargs={"k": top_k, "fetch_k": fetch_k}
                )
                retriever = base_retriever
                if use_compression:
                    compressor = LLMChainExtractor.from_llm(
                        llm_light(max_tokens=300, temperature=0)
                    )
                    retriever = ContextualCompressionRetriever(
                        base_retriever=base_retriever, base_compressor=compressor
                    )

                with st.spinner("ìš”ì•½ ì¤‘..."):
                    brief = summarize_corpus(retriever, st.session_state.pages)

                with st.spinner("ì „ëµ ë¶„ì„ ì¤‘..."):
                    strat = analyze_strategy(
                        brief=brief,
                        goal=strat_goal,
                        horizon=strat_horizon,
                        budget=strat_budget,
                        segment=strat_segment,
                        competitors=strat_competitors,
                    )
                    answer = render_strategy_md(strat)

                st.markdown("#### ğŸ§  ë‹µë³€")
                st.write(answer)
                # ì¸ìš© í˜ì´ì§€ ë¯¸ë¦¬ë³´ê¸° ìœ ì§€
                cited_pages = extract_page_citations(answer)
                if cited_pages and st.session_state.pages:
                    with st.expander(f"ğŸ” ì¸ìš©ëœ í˜ì´ì§€ ìŠ¤ë‹ˆí« ë³´ê¸° ({len(cited_pages)}ê°œ)"):
                        page_map = {pg: txt for pg, txt in st.session_state.pages}
                        for pg in cited_pages:
                            snippet = (page_map.get(pg) or "")[:500]
                            st.markdown(f"**[p.{pg}]**\n\n{snippet}")
                st.stop()  # ì»¨ì„¤íŒ… ëª¨ë“œì—ì„œëŠ” ì—¬ê¸°ì„œ ì¢…ë£Œ


            if intent == "pros_cons":
                answer = answer_pros_cons(
                    question=question,
                    retriever=retriever,
                    pages=st.session_state.pages,
                    n=n,
                    use_global=enable_global_critique,
                    per_page_chars=global_critique_pages,
                    total_chars=global_critique_total,
                    api_key=OPENAI_API_KEY,
                    max_tokens=max_tokens + 250
                )
            elif intent == "critique" and enable_global_critique and st.session_state.pages:
                answer = critique_answer_global(
                    st.session_state.pages,
                    per_page_chars=global_critique_pages,
                    total_chars=global_critique_total,
                    api_key=OPENAI_API_KEY,
                    question=question,
                    max_tokens=max_tokens + 150
                )
            else:
                chain = build_chain(retriever)
                answer = chain.invoke(question)

        st.markdown("#### ğŸ§  ë‹µë³€")
        # ë¹ˆ ë¬¸ìì—´ ê°€ë“œ
        if not (answer or "").strip():
            answer = "âš ï¸ ì‘ë‹µì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•˜ê±°ë‚˜ í† í° í•œë„ë¥¼ ë‚®ì¶°ë³´ì„¸ìš”."
        st.write(answer)

        # ê·¼ê±° ìŠ¤ë‹ˆí« ë¯¸ë¦¬ë³´ê¸°
        cited_pages = extract_page_citations(answer)
        if cited_pages and st.session_state.pages:
            with st.expander(f"ğŸ” ì¸ìš©ëœ í˜ì´ì§€ ìŠ¤ë‹ˆí« ë³´ê¸° ({len(cited_pages)}ê°œ)"):
                page_map = {pg: txt for pg, txt in st.session_state.pages}
                for pg in cited_pages:
                    snippet = (page_map.get(pg) or "")[:500]
                    st.markdown(f"**[p.{pg}]**\n\n{snippet}")
else:
    st.info("PDFë¥¼ ì—…ë¡œë“œí•˜ë©´ ì§ˆë¬¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
